import dgl.sparse as dglsp
import torch
import torch.nn as nn
import torch.nn.functional as F
from dgl.data import CoraGraphDataset
from torch.optim import Adam
import json
import dgl

import numpy as np
import dgl
import scipy.sparse as sp
import pandas as pd
import os

from sklearn.metrics.pairwise import cosine_similarity

class GATConv(nn.Module):
    def __init__(self, in_size, out_size, num_heads, dropout):
        super().__init__()

        self.out_size = out_size
        self.num_heads = num_heads

        self.dropout = nn.Dropout(dropout)
        self.W = nn.Linear(in_size, out_size * num_heads)
        self.a_l = nn.Parameter(torch.zeros(1, out_size, num_heads))
        self.a_r = nn.Parameter(torch.zeros(1, out_size, num_heads))
        self.reset_parameters()

    def reset_parameters(self):
        gain = nn.init.calculate_gain("relu")
        nn.init.xavier_normal_(self.W.weight, gain=gain)
        nn.init.xavier_normal_(self.a_l, gain=gain)
        nn.init.xavier_normal_(self.a_r, gain=gain)

    def forward(self, A_hat, Z):
        Z = self.dropout(Z)
        Z = self.W(Z).view(Z.shape[0], self.out_size, self.num_heads)

        # a^T [Wh_i || Wh_j] = a_l Wh_i + a_r Wh_j
        e_l = (Z * self.a_l).sum(dim=1)
        e_r = (Z * self.a_r).sum(dim=1)


        e = e_l[A_hat.row] + e_r[A_hat.col]

        a = F.leaky_relu(e)
        A_atten = dglsp.val_like(A_hat, a).softmax()
        a_drop = self.dropout(A_atten.val)
        A_atten = dglsp.val_like(A_atten, a_drop)
        return dglsp.bspmm(A_atten, Z)
    
class GAT(nn.Module):
    def __init__(
        self, in_size, out_size, hidden_size=8, num_heads=8, dropout=0.6
    ):
        super().__init__()

        self.in_conv = GATConv(
            in_size, hidden_size, num_heads=num_heads, dropout=dropout
        )
        self.out_conv = GATConv(
            hidden_size * num_heads, out_size, num_heads=1, dropout=dropout
        )

    def forward(self, A_hat, X):
        # Flatten the head and feature dimension.
        Z = F.elu(self.in_conv(A_hat, X)).flatten(1)
        # Average over the head dimension.
        Z = self.out_conv(A_hat, Z).mean(-1)
        return Z
    



def train(model, g, A_hat, X):
    optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)

    for epoch in range(50):
        # Forward.
        model.train()
        logits = model(A_hat, X)


        # Backward.
        optimizer.zero_grad()
        optimizer.step()

        # Compute prediction.
        model.eval()
        logits = model(A_hat, X)
        pred = logits.argmax(dim=1)


def load_graph_from_dgl(filename):
    # Đọc dữ liệu từ tệp DGL
    graph_data = dgl.load_graphs(filename)
    g = graph_data[0][0]  # Lấy đồ thị từ danh sách đồ thị đã được load

    return g


def load_data(file_path):
    """
    Đọc dữ liệu từ tệp JSON.

    Parameters:
    file_path (str): Đường dẫn đến tệp JSON chứa dữ liệu.

    Returns:
    dict: Dữ liệu được đọc từ tệp JSON.
    """
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data


def create_adjacency_matrix(g):
    src, dst = g.edges()
    src = src.numpy()
    dst = dst.numpy()
    indices = torch.tensor([src, dst], dtype=torch.long)
    N = g.num_nodes()
    A = dglsp.spmatrix(indices, shape=(N, N))
    return A


def preprocess_adjacency_matrix(A, device):
    I = dglsp.identity(A.shape, device=device)
    A_hat = A + I
    return A_hat



# Đường dẫn đến hai thư mục chứa các file JSON và DGL
json_folder = '/content/drive/MyDrive/malware_vec_2'
dgl_folder = '/content/drive/MyDrive/malware_fcg_2'
output_model_folder = '/content/drive/MyDrive/malware_gat/malware_model_2'
output_txt_folder = '/content/drive/MyDrive/malware_gat/malware_node_embedding_2'
    
def gat(json_folder, dgl_folder, output_model_folder, output_txt_folder):
    # If CUDA is available, use GPU to accelerate the training, use CPU
        # otherwise.
    dev = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # Lặp qua tất cả các file trong thư mục chứa các file JSON
    index = 0
    while True:
        json_files = sorted([f for f in os.listdir(json_folder) if f.endswith('.json')])
        dgl_files = sorted([f for f in os.listdir(dgl_folder) if f.endswith('.dgl')])


        if index >= min(len(json_files), len(dgl_files)):
            break

        json_file = json_files[index]
        dgl_file = dgl_files[index]
        print("JSON file:", json_file)
        print("DGL file:", dgl_file)

        # Tạo đường dẫn đầy đủ đến các file JSON và DGL tương ứng
        json_path = os.path.join(json_folder, json_file)
        dgl_path = os.path.join(dgl_folder, dgl_file)

        # Load dữ liệu từ file JSON và DGL
        X = load_data(json_path)
        g = load_graph_from_dgl(dgl_path)
        if g.num_nodes() < 100:
            print(f"Skipping file {json_file}: Number of nodes is less than 100")
            index += 1
            continue

        # Tiến hành huấn luyện model và lưu model đã train
        dev = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        A = create_adjacency_matrix(g)
        A_hat = preprocess_adjacency_matrix(A, dev)
        in_size = len(X[0])
        out_size = 1  # Điều chỉnh kích thước đầu ra cần thiết
        model = GAT(in_size, out_size).to(dev)
        X_tensor = torch.FloatTensor(X)
        train(model, g, A_hat, X_tensor)
        model_save_path = os.path.join(output_model_folder, f'model_{json_file.replace(".json", ".pth")}')
        torch.save(model.state_dict(), model_save_path)

        # Tạo vectơ nhúng cho các nút và lưu vào file txt
        node_embeddings = model(A_hat, X_tensor)

        # Tính toán điểm số cho tất cả các node
        reference_vector = node_embeddings.mean(dim=0)  # Chọn một vector tham chiếu
        similarities = cosine_similarity(node_embeddings.detach().cpu().numpy(), reference_vector.detach().cpu().numpy().reshape(1, -1))


        # Chọn ra 100 node có điểm số cao nhất
        top_100_indices = similarities.argsort(axis=0)[-100:][::-1].flatten()
        top_100_node_embeddings = node_embeddings[top_100_indices]

        # Lưu các node quan trọng nhất vào file txt
        top_100_node_embeddings_np = top_100_node_embeddings.detach().cpu().numpy()
        df = pd.DataFrame(top_100_node_embeddings_np)

        txt_save_path = os.path.join(output_txt_folder, f'node_embeddings_{json_file.replace(".json", ".txt")}')
        df.to_csv(txt_save_path, index=False, header=False, sep='\n')
        print(f"done {index}")
        index += 1

    print("Hoàn thành!")


##



